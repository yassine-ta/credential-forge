#!/usr/bin/env python3
"""Demonstration of CredentialForge Local Project Setup."""

import sys
from pathlib import Path

# Add the project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from credentialforge.llm.llama_interface import LlamaInterface


def demonstrate_local_project():
    """Demonstrate the local project setup."""
    print("ğŸ  CredentialForge Local Project Demonstration")
    print("=" * 60)
    print()
    
    # Show project structure
    print("ğŸ“ Local Project Structure:")
    print("E:\\credential_forge\\")
    print("â”œâ”€â”€ models/          # LLM models (downloaded locally)")
    print("â”œâ”€â”€ cache/           # Caching and temporary files")
    print("â”œâ”€â”€ config/          # Configuration files")
    print("â”œâ”€â”€ logs/            # Log files")
    print("â”œâ”€â”€ output/          # Generated synthetic documents")
    print("â”œâ”€â”€ data/            # Data files (regex database, etc.)")
    print("â”œâ”€â”€ templates/       # Template files")
    print("â””â”€â”€ credentialforge/ # Main application code")
    print()
    
    # Show local configuration
    print("âš™ï¸  Local Configuration:")
    config_file = project_root / "config" / "local_config.json"
    if config_file.exists():
        import json
        with open(config_file, 'r') as f:
            config = json.load(f)
        
        for key, value in config.items():
            if key != "description":
                print(f"  â€¢ {key}: {value}")
    print()
    
    # Show available local models
    print("ğŸ¤– Local LLM Models:")
    available_models = LlamaInterface.list_available_models()
    if available_models:
        for model in available_models:
            model_path = project_root / "models" / f"{model}.gguf"
            if model_path.exists():
                size_mb = model_path.stat().st_size / (1024 * 1024)
                print(f"  âœ… {model} ({size_mb:.1f} MB) - Local")
            else:
                print(f"  â¬‡ï¸  {model} - Available for download")
    else:
        print("  ğŸ“¥ No models downloaded yet")
        print("  Models will be downloaded to ./models/ when selected")
    print()
    
    # Show local directories
    print("ğŸ“‚ Local Directories Status:")
    directories = ["models", "cache", "config", "logs", "output", "data", "templates"]
    for directory in directories:
        dir_path = project_root / directory
        if dir_path.exists():
            file_count = len(list(dir_path.iterdir()))
            print(f"  âœ… {directory}/ ({file_count} items)")
        else:
            print(f"  âŒ {directory}/ (not found)")
    print()
    
    # Show local operation benefits
    print("ğŸ¯ Local Operation Benefits:")
    print("  âœ… All files stored in project directory")
    print("  âœ… No system-wide dependencies")
    print("  âœ… No global configuration files")
    print("  âœ… Completely portable")
    print("  âœ… LLM models downloaded locally")
    print("  âœ… All caches stored locally")
    print("  âœ… All logs written locally")
    print("  âœ… All generated files saved locally")
    print()
    
    # Show usage examples
    print("ğŸš€ Local Usage Examples:")
    print("  # Interactive mode with local models")
    print("  python -m credentialforge interactive")
    print()
    print("  # Generate files with local storage")
    print("  python -m credentialforge generate --output-dir ./output --num-files 5")
    print()
    print("  # List available local models")
    print("  python -c \"from credentialforge.llm.llama_interface import LlamaInterface; print(LlamaInterface.list_available_models())\"")
    print()
    
    # Show model download capability
    print("ğŸ“¥ Model Download Capability:")
    print("  When you select a model in interactive mode:")
    print("  1. System checks ./models/ directory")
    print("  2. If model not found, downloads to ./models/")
    print("  3. Model is stored locally for future use")
    print("  4. No internet required after initial download")
    print()
    
    print("ğŸ‰ CredentialForge is now completely local!")
    print("All downloads, caches, and dependencies are stored in the current folder.")
    print("=" * 60)


def show_local_file_examples():
    """Show examples of local file operations."""
    print("\nğŸ“„ Local File Examples:")
    print("-" * 30)
    
    # Show local config
    config_file = project_root / "config" / "local_config.json"
    if config_file.exists():
        print(f"âœ… Local config: {config_file}")
    
    # Show local data
    data_file = project_root / "data" / "regex_db.json"
    if data_file.exists():
        print(f"âœ… Local regex database: {data_file}")
    
    # Show local output
    output_dir = project_root / "output"
    if output_dir.exists():
        output_files = list(output_dir.glob("*"))
        if output_files:
            print(f"âœ… Local output files: {len(output_files)} files in {output_dir}")
        else:
            print(f"ğŸ“ Local output directory: {output_dir} (empty)")
    
    # Show local logs
    logs_dir = project_root / "logs"
    if logs_dir.exists():
        log_files = list(logs_dir.glob("*.log"))
        if log_files:
            print(f"âœ… Local log files: {len(log_files)} files in {logs_dir}")
        else:
            print(f"ğŸ“ Local logs directory: {logs_dir} (empty)")
    
    # Show local models
    models_dir = project_root / "models"
    if models_dir.exists():
        model_files = list(models_dir.glob("*.gguf"))
        if model_files:
            print(f"âœ… Local model files: {len(model_files)} files in {models_dir}")
        else:
            print(f"ğŸ“ Local models directory: {models_dir} (empty)")


if __name__ == "__main__":
    try:
        demonstrate_local_project()
        show_local_file_examples()
        
        print("\nğŸš€ To start using CredentialForge locally:")
        print("   python -m credentialforge interactive")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
